{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfbc4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import markdown\n",
    "import io\n",
    "import re\n",
    "from dateutil import parser\n",
    "import calendar\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2db4fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************* {'this_quarter': 2, 'previous_quarter': 3, 'same_q_last_year': 4}\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import calendar\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "MAX_SCAN_ROWS = 6\n",
    "\n",
    "TARGET_DATES = {\n",
    "    \"this_quarter\": date(2025, 9, 30),\n",
    "    \"previous_quarter\": date(2025, 6, 30),\n",
    "    \"same_q_last_year\": date(2024, 9, 30)\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# REGEX - ADDED MONTH-YEAR PATTERN\n",
    "# -----------------------------\n",
    "FULL_DATE_REGEX = re.compile(r'''\n",
    "    \\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}                     # 30.09.2025\n",
    "    |\n",
    "    \\d{1,2}\\s*[-]\\s*(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\\s*[-]\\s*\\d{2,4}  # 30-Sep-25\n",
    "    |\n",
    "    \\d{1,2}(?:st|nd|rd|th)?\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s*,?\\s*\\d{4}  # 30th September, 2025\n",
    "    |\n",
    "    (January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(?:st|nd|rd|th)?\\s*,?\\s*\\d{4}  # September 30th, 2025\n",
    "''', re.IGNORECASE | re.VERBOSE)\n",
    "\n",
    "# NEW: Month-Year only pattern\n",
    "MONTH_YEAR_REGEX = re.compile(r'''\n",
    "    (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\\s*,?\\s*\\d{4}  # Sept, 2025 or Sept 2025\n",
    "    |\n",
    "    (January|February|March|April|May|June|July|August|September|October|November|December)\\s*,?\\s*\\d{4}  # September, 2025\n",
    "''', re.IGNORECASE | re.VERBOSE)\n",
    "\n",
    "MONTH_MAP = {\n",
    "    'jan': 1, 'january': 1, 'feb': 2, 'february': 2, 'mar': 3, 'march': 3,\n",
    "    'apr': 4, 'april': 4, 'may': 5, 'jun': 6, 'june': 6, 'jul': 7, 'july': 7,\n",
    "    'aug': 8, 'august': 8, 'sep': 9, 'sept': 9, 'september': 9,\n",
    "    'oct': 10, 'october': 10, 'nov': 11, 'november': 11, 'dec': 12, 'december': 12\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# CLEANING\n",
    "# -----------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text while preserving date information\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Handle typos\n",
    "    text = re.sub(r'3lst', '31st', text)\n",
    "    text = re.sub(r'(\\d)lst\\b', r'\\1st', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    \n",
    "    # Remove markdown and formatting\n",
    "    text = re.sub(r'\\*\\*|__|`', '', text)\n",
    "    \n",
    "    # Remove audit annotations\n",
    "    text = re.sub(r'\\(?\\s*(Unaudited|Audited)\\s*\\)?', ' ', text, flags=re.I)\n",
    "    \n",
    "    # Remove extra phrases (but keep ordinal suffixes for date extraction)\n",
    "    text = re.sub(r'Quarter ended|Six Months ended|Year ended|ended|Refer note \\d+', ' ', text, flags=re.I)\n",
    "    \n",
    "    # Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# DATE EXTRACTION\n",
    "# -----------------------------\n",
    "def extract_all_date_strings(text: str):\n",
    "    \"\"\"Extract all potential date strings from text\"\"\"\n",
    "    dates = []\n",
    "    \n",
    "    # First try to find full dates (with day)\n",
    "    for match in FULL_DATE_REGEX.finditer(text):\n",
    "        dates.append(match.group(0).strip())\n",
    "    \n",
    "    # If no full dates found, try month-year only\n",
    "    if not dates:\n",
    "        for match in MONTH_YEAR_REGEX.finditer(text):\n",
    "            dates.append(match.group(0).strip())\n",
    "    \n",
    "    return dates\n",
    "\n",
    "# -----------------------------\n",
    "# DATE PARSING\n",
    "# -----------------------------\n",
    "def parse_date(date_str: str):\n",
    "    \"\"\"Parse date string to date object\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Remove ordinal suffixes before parsing\n",
    "        date_str_clean = re.sub(r'(\\d+)(st|nd|rd|th)\\b', r'\\1', date_str, flags=re.I)\n",
    "        date_str_clean = date_str_clean.strip()\n",
    "        \n",
    "        # Try DD.MM.YYYY or DD/MM/YYYY or DD-MM-YYYY format\n",
    "        match = re.match(r'^(\\d{1,2})[./-](\\d{1,2})[./-](\\d{2,4})$', date_str_clean)\n",
    "        if match:\n",
    "            day, month, year = match.groups()\n",
    "            year = int(year)\n",
    "            if year < 100:\n",
    "                year += 2000 if year < 50 else 1900\n",
    "            return date(year, int(month), int(day))\n",
    "        \n",
    "        # Try DD-Month-YYYY format (30-September-2025)\n",
    "        match = re.match(r'^(\\d{1,2})\\s*[-]\\s*([A-Za-z]+)\\s*[-]\\s*(\\d{2,4})$', date_str_clean)\n",
    "        if match:\n",
    "            day, month_str, year = match.groups()\n",
    "            month_key = month_str[:4].lower()\n",
    "            month = MONTH_MAP.get(month_key) or MONTH_MAP.get(month_str.lower())\n",
    "            if month:\n",
    "                year = int(year)\n",
    "                if year < 100:\n",
    "                    year += 2000 if year < 50 else 1900\n",
    "                return date(year, month, int(day))\n",
    "        \n",
    "        # Try DD Month YYYY format (30 September 2025 or 30 September, 2025)\n",
    "        match = re.match(r'^(\\d{1,2})\\s+([A-Za-z]+)\\s*,?\\s*(\\d{4})$', date_str_clean, re.I)\n",
    "        if match:\n",
    "            day, month_str, year = match.groups()\n",
    "            month_key = month_str[:4].lower()\n",
    "            month = MONTH_MAP.get(month_key) or MONTH_MAP.get(month_str.lower())\n",
    "            if month:\n",
    "                return date(int(year), month, int(day))\n",
    "        \n",
    "        # Try Month DD YYYY format (September 30 2025 or September 30, 2025)\n",
    "        match = re.match(r'^([A-Za-z]+)\\s+(\\d{1,2})\\s*,?\\s*(\\d{4})$', date_str_clean, re.I)\n",
    "        if match:\n",
    "            month_str, day, year = match.groups()\n",
    "            month_key = month_str[:4].lower()\n",
    "            month = MONTH_MAP.get(month_key) or MONTH_MAP.get(month_str.lower())\n",
    "            if month:\n",
    "                return date(int(year), month, int(day))\n",
    "        \n",
    "        # Try Month YYYY format (Sept, 2025 or September 2025) - infer last day of month\n",
    "        match = re.match(r'^([A-Za-z]+)\\s*,?\\s*(\\d{4})$', date_str_clean, re.I)\n",
    "        if match:\n",
    "            month_str, year = match.groups()\n",
    "            month_key = month_str[:4].lower()\n",
    "            month = MONTH_MAP.get(month_key) or MONTH_MAP.get(month_str.lower())\n",
    "            if month:\n",
    "                year = int(year)\n",
    "                # Get last day of the month\n",
    "                last_day = calendar.monthrange(year, month)[1]\n",
    "                return date(year, month, last_day)\n",
    "        \n",
    "        # Fallback to dateutil\n",
    "        dt = parser.parse(date_str_clean, dayfirst=True, fuzzy=True)\n",
    "        return dt.date()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN FUNCTION - SCAN HEADERS AND ROWS\n",
    "# -----------------------------\n",
    "def find_date_columns(df, target_dates=TARGET_DATES, max_rows=MAX_SCAN_ROWS, debug=False):\n",
    "    \"\"\"\n",
    "    Scan column headers AND first few rows to find which column contains each target date.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        target_dates: dict of {name: date_object}\n",
    "        max_rows: number of rows to scan\n",
    "        debug: print debug information\n",
    "    \n",
    "    Returns:\n",
    "        dict: {date_name: column_index}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Scanning column headers and first {max_rows} rows...\")\n",
    "        print(f\"Looking for dates: {target_dates}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # STEP 1: Scan column headers\n",
    "    if debug:\n",
    "        print(\"STEP 1: Scanning column headers...\")\n",
    "    \n",
    "    for col_idx, col_name in enumerate(df.columns):\n",
    "        col_str = str(col_name)\n",
    "        cleaned = clean_text(col_str)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Column {col_idx}: '{col_str}'\")\n",
    "        \n",
    "        date_strings = extract_all_date_strings(cleaned)\n",
    "        \n",
    "        if date_strings:\n",
    "            if debug:\n",
    "                print(f\"  Found date strings: {date_strings}\")\n",
    "            \n",
    "            for date_str in date_strings:\n",
    "                parsed_date = parse_date(date_str)\n",
    "                \n",
    "                if debug and parsed_date:\n",
    "                    print(f\"  Parsed '{date_str}' → {parsed_date}\")\n",
    "                \n",
    "                if parsed_date:\n",
    "                    for date_name, target_date in target_dates.items():\n",
    "                        if parsed_date == target_date and date_name not in result:\n",
    "                            result[date_name] = col_idx\n",
    "                            if debug:\n",
    "                                print(f\"  ✓ MATCH! {date_name} found in column {col_idx}\")\n",
    "    \n",
    "    # STEP 2: Scan first few rows (for multi-row headers)\n",
    "    if debug:\n",
    "        print(f\"\\nSTEP 2: Scanning first {max_rows} rows...\")\n",
    "    \n",
    "    for row_idx in range(min(max_rows, len(df))):\n",
    "        if debug:\n",
    "            print(f\"\\nRow {row_idx}:\")\n",
    "        \n",
    "        for col_idx in range(len(df.columns)):\n",
    "            cell = df.iloc[row_idx, col_idx]\n",
    "            \n",
    "            # Skip NaN and non-string cells\n",
    "            if pd.isna(cell):\n",
    "                continue\n",
    "            \n",
    "            cell_str = str(cell)\n",
    "            cleaned = clean_text(cell_str)\n",
    "            \n",
    "            if not cleaned:\n",
    "                continue\n",
    "            \n",
    "            # Look for dates\n",
    "            date_strings = extract_all_date_strings(cleaned)\n",
    "            \n",
    "            if date_strings:\n",
    "                if debug:\n",
    "                    print(f\"  Col {col_idx}: '{cell_str[:60]}...' → found {date_strings}\")\n",
    "                \n",
    "                for date_str in date_strings:\n",
    "                    parsed_date = parse_date(date_str)\n",
    "                    \n",
    "                    if debug and parsed_date:\n",
    "                        print(f\"    Parsed '{date_str}' → {parsed_date}\")\n",
    "                    \n",
    "                    if parsed_date:\n",
    "                        for date_name, target_date in target_dates.items():\n",
    "                            if parsed_date == target_date and date_name not in result:\n",
    "                                result[date_name] = col_idx\n",
    "                                if debug:\n",
    "                                    print(f\"    ✓ MATCH! {date_name} found in column {col_idx}\")\n",
    "        \n",
    "        # Early exit if all found\n",
    "        if len(result) == len(target_dates):\n",
    "            if debug:\n",
    "                print(\"\\nAll dates found! Stopping scan.\")\n",
    "            break\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"Final result: {result}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# -----------------------------\n",
    "# CONVENIENCE FUNCTION\n",
    "# -----------------------------\n",
    "def get_column_indices(df, max_rows=MAX_SCAN_ROWS, debug=False):\n",
    "    \"\"\"\n",
    "    Get column indices for target dates.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (this_quarter_column, previous_quarter_column, same_q_last_year_column)\n",
    "    \"\"\"\n",
    "    columns = find_date_columns(df, max_rows=max_rows, debug=debug)\n",
    "    \n",
    "    this_quarter_column = columns.get('this_quarter')\n",
    "    previous_quarter_column = columns.get('previous_quarter')\n",
    "    same_q_last_year_column = columns.get('same_q_last_year')\n",
    "    \n",
    "    return this_quarter_column, previous_quarter_column, same_q_last_year_column\n",
    "\n",
    "# -----------------------------\n",
    "# USAGE\n",
    "# -----------------------------\n",
    "def get_column_index(df, debug=False):\n",
    "    \"\"\"Main function to get column indices\"\"\"\n",
    "    \n",
    "    # Find columns\n",
    "    columns = find_date_columns(df, debug=debug)\n",
    "    \n",
    "    # Get individual variables\n",
    "    this_quarter_column, previous_quarter_column, same_q_last_year_column = get_column_indices(df, debug=debug)\n",
    "\n",
    "    return this_quarter_column, previous_quarter_column, same_q_last_year_column\n",
    "\n",
    "# -----------------------------\n",
    "# TEST\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with your second table structure\n",
    "    markdown_file_path = \"saved_md_files/d60ccb02-681e-447e-bb12-263e2ded9547_10.md\"\n",
    "    # 1. Load the markdown file content\n",
    "    with open(markdown_file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    # 2. Convert text to HTML (ensure 'tables' extension is included)\n",
    "    table = markdown.markdown(text, extensions=['tables'])\n",
    "    # 3. Parse HTML with Pandas\n",
    "    # read_html returns a list of DataFrames, so we take the first one\n",
    "    df = pd.read_html(io.StringIO(table))[0]\n",
    "    #print(df.iloc[0:4,:])\n",
    "    \n",
    "    # Find columns\n",
    "    columns = find_date_columns(df, debug=False)\n",
    "    print(\"*********************\", columns)\n",
    "    \n",
    "    # Get individual variables\n",
    "    this_quarter_column, previous_quarter_column, same_q_last_year_column = get_column_index(df, debug=False)\n",
    "    print(this_quarter_column)\n",
    "    print(previous_quarter_column)\n",
    "    print(same_q_last_year_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9140734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************* {}\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import calendar\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "MAX_SCAN_ROWS = 6\n",
    "\n",
    "TARGET_DATES = {\n",
    "    \"this_quarter\": date(2025, 9, 30),\n",
    "    \"previous_quarter\": date(2025, 6, 30),\n",
    "    \"same_q_last_year\": date(2024, 9, 30)\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# REGEX - FIXED TO HANDLE \"30th September, 2025\"\n",
    "# -----------------------------\n",
    "FULL_DATE_REGEX = re.compile(r'''\n",
    "    \\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}                     # 30.09.2025\n",
    "    |\n",
    "    \\d{1,2}\\s*[-]\\s*(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\\s*[-]\\s*\\d{2,4}  # 30-Sep-25\n",
    "    |\n",
    "    \\d{1,2}(?:st|nd|rd|th)?\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s*,?\\s*\\d{4}  # 30th September, 2025\n",
    "    |\n",
    "    (January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(?:st|nd|rd|th)?\\s*,?\\s*\\d{4}  # September 30th, 2025\n",
    "''', re.IGNORECASE | re.VERBOSE)\n",
    "\n",
    "MONTH_MAP = {\n",
    "    'jan': 1, 'january': 1, 'feb': 2, 'february': 2, 'mar': 3, 'march': 3,\n",
    "    'apr': 4, 'april': 4, 'may': 5, 'jun': 6, 'june': 6, 'jul': 7, 'july': 7,\n",
    "    'aug': 8, 'august': 8, 'sep': 9, 'sept': 9, 'september': 9,\n",
    "    'oct': 10, 'october': 10, 'nov': 11, 'november': 11, 'dec': 12, 'december': 12\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# CLEANING\n",
    "# -----------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text while preserving date information\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Handle typos\n",
    "    text = re.sub(r'3lst', '31st', text)\n",
    "    text = re.sub(r'(\\d)lst\\b', r'\\1st', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    \n",
    "    # Remove markdown and formatting\n",
    "    text = re.sub(r'\\*\\*|__|`', '', text)\n",
    "    \n",
    "    # Remove audit annotations\n",
    "    text = re.sub(r'\\(?\\s*(Unaudited|Audited)\\s*\\)?', ' ', text, flags=re.I)\n",
    "    \n",
    "    # Remove extra phrases (but keep ordinal suffixes for date extraction)\n",
    "    text = re.sub(r'Quarter ended|Six Months ended|Year ended|ended|Refer note \\d+', ' ', text, flags=re.I)\n",
    "    \n",
    "    # Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# DATE EXTRACTION\n",
    "# -----------------------------\n",
    "def extract_all_date_strings(text: str):\n",
    "    \"\"\"Extract all potential date strings from text\"\"\"\n",
    "    dates = []\n",
    "    for match in FULL_DATE_REGEX.finditer(text):\n",
    "        dates.append(match.group(0).strip())\n",
    "    return dates\n",
    "\n",
    "# -----------------------------\n",
    "# DATE PARSING\n",
    "# -----------------------------\n",
    "def parse_date(date_str: str):\n",
    "    \"\"\"Parse date string to date object\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Remove ordinal suffixes before parsing\n",
    "        date_str_clean = re.sub(r'(\\d+)(st|nd|rd|th)\\b', r'\\1', date_str, flags=re.I)\n",
    "        date_str_clean = date_str_clean.strip()\n",
    "        \n",
    "        # Try DD.MM.YYYY or DD/MM/YYYY or DD-MM-YYYY format\n",
    "        match = re.match(r'^(\\d{1,2})[./-](\\d{1,2})[./-](\\d{2,4})$', date_str_clean)\n",
    "        if match:\n",
    "            day, month, year = match.groups()\n",
    "            year = int(year)\n",
    "            if year < 100:\n",
    "                year += 2000 if year < 50 else 1900\n",
    "            return date(year, int(month), int(day))\n",
    "        \n",
    "        # Try DD-Month-YYYY format (30-September-2025)\n",
    "        match = re.match(r'^(\\d{1,2})\\s*[-]\\s*([A-Za-z]+)\\s*[-]\\s*(\\d{2,4})$', date_str_clean)\n",
    "        if match:\n",
    "            day, month_str, year = match.groups()\n",
    "            month_key = month_str[:4].lower()\n",
    "            month = MONTH_MAP.get(month_key) or MONTH_MAP.get(month_str.lower())\n",
    "            if month:\n",
    "                year = int(year)\n",
    "                if year < 100:\n",
    "                    year += 2000 if year < 50 else 1900\n",
    "                return date(year, month, int(day))\n",
    "        \n",
    "        # Try DD Month YYYY format (30 September 2025 or 30 September, 2025)\n",
    "        match = re.match(r'^(\\d{1,2})\\s+([A-Za-z]+)\\s*,?\\s*(\\d{4})$', date_str_clean, re.I)\n",
    "        if match:\n",
    "            day, month_str, year = match.groups()\n",
    "            month_key = month_str[:4].lower()\n",
    "            month = MONTH_MAP.get(month_key) or MONTH_MAP.get(month_str.lower())\n",
    "            if month:\n",
    "                return date(int(year), month, int(day))\n",
    "        \n",
    "        # Try Month DD YYYY format (September 30 2025 or September 30, 2025)\n",
    "        match = re.match(r'^([A-Za-z]+)\\s+(\\d{1,2})\\s*,?\\s*(\\d{4})$', date_str_clean, re.I)\n",
    "        if match:\n",
    "            month_str, day, year = match.groups()\n",
    "            month_key = month_str[:4].lower()\n",
    "            month = MONTH_MAP.get(month_key) or MONTH_MAP.get(month_str.lower())\n",
    "            if month:\n",
    "                return date(int(year), month, int(day))\n",
    "        \n",
    "        # Fallback to dateutil\n",
    "        dt = parser.parse(date_str_clean, dayfirst=True, fuzzy=True)\n",
    "        return dt.date()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN FUNCTION - SCAN HEADERS AND ROWS\n",
    "# -----------------------------\n",
    "def find_date_columns(df, target_dates=TARGET_DATES, max_rows=MAX_SCAN_ROWS, debug=False):\n",
    "    \"\"\"\n",
    "    Scan column headers AND first few rows to find which column contains each target date.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        target_dates: dict of {name: date_object}\n",
    "        max_rows: number of rows to scan\n",
    "        debug: print debug information\n",
    "    \n",
    "    Returns:\n",
    "        dict: {date_name: column_index}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Scanning column headers and first {max_rows} rows...\")\n",
    "        print(f\"Looking for dates: {target_dates}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # STEP 1: Scan column headers\n",
    "    if debug:\n",
    "        print(\"STEP 1: Scanning column headers...\")\n",
    "    \n",
    "    for col_idx, col_name in enumerate(df.columns):\n",
    "        col_str = str(col_name)\n",
    "        cleaned = clean_text(col_str)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Column {col_idx}: '{col_str}'\")\n",
    "        \n",
    "        date_strings = extract_all_date_strings(cleaned)\n",
    "        \n",
    "        if date_strings:\n",
    "            if debug:\n",
    "                print(f\"  Found date strings: {date_strings}\")\n",
    "            \n",
    "            for date_str in date_strings:\n",
    "                parsed_date = parse_date(date_str)\n",
    "                \n",
    "                if debug and parsed_date:\n",
    "                    print(f\"  Parsed '{date_str}' → {parsed_date}\")\n",
    "                \n",
    "                if parsed_date:\n",
    "                    for date_name, target_date in target_dates.items():\n",
    "                        if parsed_date == target_date and date_name not in result:\n",
    "                            result[date_name] = col_idx\n",
    "                            if debug:\n",
    "                                print(f\"  ✓ MATCH! {date_name} found in column {col_idx}\")\n",
    "    \n",
    "    # STEP 2: Scan first few rows (for multi-row headers)\n",
    "    if debug:\n",
    "        print(f\"\\nSTEP 2: Scanning first {max_rows} rows...\")\n",
    "    \n",
    "    for row_idx in range(min(max_rows, len(df))):\n",
    "        if debug:\n",
    "            print(f\"\\nRow {row_idx}:\")\n",
    "        \n",
    "        for col_idx in range(len(df.columns)):\n",
    "            cell = df.iloc[row_idx, col_idx]\n",
    "            \n",
    "            # Skip NaN and non-string cells\n",
    "            if pd.isna(cell):\n",
    "                continue\n",
    "            \n",
    "            cell_str = str(cell)\n",
    "            cleaned = clean_text(cell_str)\n",
    "            \n",
    "            if not cleaned:\n",
    "                continue\n",
    "            \n",
    "            # Look for dates\n",
    "            date_strings = extract_all_date_strings(cleaned)\n",
    "            \n",
    "            if date_strings:\n",
    "                if debug:\n",
    "                    print(f\"  Col {col_idx}: '{cell_str[:60]}...' → found {date_strings}\")\n",
    "                \n",
    "                for date_str in date_strings:\n",
    "                    parsed_date = parse_date(date_str)\n",
    "                    \n",
    "                    if debug and parsed_date:\n",
    "                        print(f\"    Parsed '{date_str}' → {parsed_date}\")\n",
    "                    \n",
    "                    if parsed_date:\n",
    "                        for date_name, target_date in target_dates.items():\n",
    "                            if parsed_date == target_date and date_name not in result:\n",
    "                                result[date_name] = col_idx\n",
    "                                if debug:\n",
    "                                    print(f\"    ✓ MATCH! {date_name} found in column {col_idx}\")\n",
    "        \n",
    "        # Early exit if all found\n",
    "        if len(result) == len(target_dates):\n",
    "            if debug:\n",
    "                print(\"\\nAll dates found! Stopping scan.\")\n",
    "            break\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"Final result: {result}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# -----------------------------\n",
    "# CONVENIENCE FUNCTION\n",
    "# -----------------------------\n",
    "def get_column_indices(df, max_rows=MAX_SCAN_ROWS, debug=False):\n",
    "    \"\"\"\n",
    "    Get column indices for target dates.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (this_quarter_column, previous_quarter_column, same_q_last_year_column)\n",
    "    \"\"\"\n",
    "    columns = find_date_columns(df, max_rows=max_rows, debug=debug)\n",
    "    \n",
    "    this_quarter_column = columns.get('this_quarter')\n",
    "    previous_quarter_column = columns.get('previous_quarter')\n",
    "    same_q_last_year_column = columns.get('same_q_last_year')\n",
    "    \n",
    "    return this_quarter_column, previous_quarter_column, same_q_last_year_column\n",
    "\n",
    "# -----------------------------\n",
    "# USAGE\n",
    "# -----------------------------\n",
    "def get_column_index(df, debug=False):\n",
    "    \"\"\"Main function to get column indices\"\"\"\n",
    "    \n",
    "    # Find columns\n",
    "    columns = find_date_columns(df, debug=debug)\n",
    "    \n",
    "    # Get individual variables\n",
    "    this_quarter_column, previous_quarter_column, same_q_last_year_column = get_column_indices(df, debug=debug)\n",
    "\n",
    "    return this_quarter_column, previous_quarter_column, same_q_last_year_column\n",
    "\n",
    "# -----------------------------\n",
    "# TEST\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with your second table structure\n",
    "    markdown_file_path = \"saved_md_files/aab996e0-1774-4dcd-9c96-576c67805cd0_9.md\"\n",
    "    # 1. Load the markdown file content\n",
    "    with open(markdown_file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    # 2. Convert text to HTML (ensure 'tables' extension is included)\n",
    "    table = markdown.markdown(text, extensions=['tables'])\n",
    "    # 3. Parse HTML with Pandas\n",
    "    # read_html returns a list of DataFrames, so we take the first one\n",
    "    df = pd.read_html(io.StringIO(table))[0]\n",
    "    #print(df.iloc[0:4,:])\n",
    "    \n",
    "    # Find columns\n",
    "    columns = find_date_columns(df, debug=False)\n",
    "    print(\"*********************\", columns)\n",
    "    \n",
    "    # Get individual variables\n",
    "    this_quarter_column, previous_quarter_column, same_q_last_year_column = get_column_index(df, debug=False)\n",
    "    print(this_quarter_column)\n",
    "    print(previous_quarter_column)\n",
    "    print(same_q_last_year_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3e2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f44808af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Variables:\n",
      "revenue_row = 2\n",
      "expenses_row = 13\n",
      "pbt_row = 14\n",
      "comprehensive_income_row = 26\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "TARGET_ROW_PATTERNS = {\n",
    "    \"revenue_from_operations\": {\n",
    "        \"must_have\": [\"revenue from operations\"],\n",
    "        \"prefer\": [\"operations\"],\n",
    "        \"exclude\": [\"total\", \"other\"]\n",
    "    },\n",
    "    \"total_expenses\": {\n",
    "        \"must_have\": [\"total expense\"],\n",
    "        \"prefer\": [\"total\"],\n",
    "        \"exclude\": []\n",
    "    },\n",
    "    \"profit_before_tax\": {\n",
    "        \"must_have\": [\"profit\", \"tax\"],\n",
    "        \"prefer\": [\"before\"],\n",
    "        \"exclude\": [\"comprehensive\", \"after\"]\n",
    "    },\n",
    "    \"total_comprehensive_income\": {\n",
    "        \"must_have\": [\"total comprehensive income\"],\n",
    "        \"prefer\": [\"total\"],\n",
    "        \"exclude\": [\"other\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# FIXED TEXT CLEANING\n",
    "# -----------------------------\n",
    "def clean_row_text(text: str) -> str:\n",
    "    \"\"\"Clean text for row matching\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove markdown formatting\n",
    "    text = re.sub(r'\\*\\*|__|`', '', text)\n",
    "    \n",
    "    # Remove parentheses content like (Refer Note 5), (IV), etc\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    \n",
    "    # Remove common prefixes like \"a.\", \"(a)\", \"i.\", \"1.\", etc\n",
    "    text = re.sub(r'^[a-z]\\.\\s*', '', text)\n",
    "    text = re.sub(r'^\\([a-z]\\)\\s*', '', text)\n",
    "    text = re.sub(r'^\\d+\\.\\s*', '', text)\n",
    "    \n",
    "    # Normalize separators\n",
    "    text = re.sub(r'[/]+', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# SIMPLIFIED MATCHING\n",
    "# -----------------------------\n",
    "def matches_pattern(text: str, pattern_config: dict, debug=False) -> bool:\n",
    "    \"\"\"Check if text matches pattern based on keywords.\"\"\"\n",
    "    cleaned = clean_row_text(text)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"      Checking: '{text[:60]}...' → cleaned: '{cleaned[:60]}...'\")\n",
    "    \n",
    "    # Check must-have keywords (ALL required)\n",
    "    must_have = pattern_config.get(\"must_have\", [])\n",
    "    for keyword in must_have:\n",
    "        if keyword not in cleaned:\n",
    "            if debug:\n",
    "                print(f\"        ✗ Missing required keyword: '{keyword}'\")\n",
    "            return False\n",
    "    \n",
    "    # Check exclude keywords (NONE allowed)\n",
    "    exclude = pattern_config.get(\"exclude\", [])\n",
    "    for keyword in exclude:\n",
    "        if keyword in cleaned:\n",
    "            if debug:\n",
    "                print(f\"        ✗ Contains excluded keyword: '{keyword}'\")\n",
    "            return False\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"        ✓ Match!\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN FUNCTION\n",
    "# -----------------------------\n",
    "def find_row_indices(df, target_patterns=TARGET_ROW_PATTERNS, \n",
    "                     search_columns=5, max_rows=None, debug=True):\n",
    "    \"\"\"\n",
    "    Find row index for first occurrence of each pattern.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    max_rows = max_rows or len(df)\n",
    "    max_cols = min(search_columns, len(df.columns))\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Searching for row patterns in first {max_cols} columns...\")\n",
    "        print(f\"Max rows to search: {max_rows}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # For each target pattern, find FIRST occurrence\n",
    "    for pattern_name, pattern_config in target_patterns.items():\n",
    "        found = False\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Searching for: {pattern_name}\")\n",
    "            print(f\"  Must have ALL: {pattern_config.get('must_have', [])}\")\n",
    "            print(f\"  Exclude: {pattern_config.get('exclude', [])}\")\n",
    "        \n",
    "        # Scan rows (stop at first match)\n",
    "        for row_idx in range(min(max_rows, len(df))):\n",
    "            \n",
    "            if found:\n",
    "                break\n",
    "            \n",
    "            # Check across first N columns\n",
    "            for col_idx in range(max_cols):\n",
    "                cell = df.iloc[row_idx, col_idx]\n",
    "                \n",
    "                if pd.isna(cell):\n",
    "                    continue\n",
    "                \n",
    "                cell_str = str(cell).strip()\n",
    "                \n",
    "                if not cell_str:\n",
    "                    continue\n",
    "                \n",
    "                # Check if this cell matches the pattern\n",
    "                if matches_pattern(cell_str, pattern_config, debug=debug):\n",
    "                    result[pattern_name] = row_idx\n",
    "                    found = True\n",
    "                    \n",
    "                    if debug:\n",
    "                        print(f\"  ✓ FOUND at Row {row_idx}, Col {col_idx}\")\n",
    "                        print(f\"    Original text: '{cell_str[:80]}'\\n\")\n",
    "                    \n",
    "                    break  # Stop searching columns\n",
    "        \n",
    "        if not found and debug:\n",
    "            print(f\"  ✗ Not found\\n\")\n",
    "    \n",
    "    if debug:\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Final result: {result}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# -----------------------------\n",
    "# CONVENIENCE FUNCTION\n",
    "# -----------------------------\n",
    "def get_row_indices(df, search_columns=5, debug=False):\n",
    "    \"\"\"\n",
    "    Get row indices for common financial statement items.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (revenue_row, expenses_row, pbt_row, comprehensive_income_row)\n",
    "    \"\"\"\n",
    "    rows = find_row_indices(df, search_columns=search_columns, debug=debug)\n",
    "    \n",
    "    revenue_row = rows.get('revenue_from_operations')\n",
    "    expenses_row = rows.get('total_expenses')\n",
    "    pbt_row = rows.get('profit_before_tax')\n",
    "    comprehensive_income_row = rows.get('total_comprehensive_income')\n",
    "    \n",
    "    return revenue_row, expenses_row, pbt_row, comprehensive_income_row\n",
    "\n",
    "\n",
    "markdown_file_path = \"saved_md_files/2cec8dbb-f9ba-4fc7-b25f-9e019a185852_4.md\"\n",
    "# 1. Load the markdown file content\n",
    "with open(markdown_file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "# 2. Convert text to HTML (ensure 'tables' extension is included)\n",
    "table = markdown.markdown(text, extensions=['tables'])\n",
    "# 3. Parse HTML with Pandas\n",
    "# read_html returns a list of DataFrames, so we take the first one\n",
    "df = pd.read_html(io.StringIO(table))[0]\n",
    "#print(df.iloc[0:4,:])\n",
    "\n",
    "\n",
    "w,x,y,z = get_row_indices(df, search_columns=5, debug=False)\n",
    "\n",
    "print(\"\\nFinal Variables:\")\n",
    "print(f\"revenue_row = {w}\")\n",
    "print(f\"expenses_row = {x}\")\n",
    "print(f\"pbt_row = {y}\")\n",
    "print(f\"comprehensive_income_row = {z}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8681d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd14f3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Using convenience function:\n",
      "revenue_row = 0\n",
      "expenses_row = 14\n",
      "pbt_row = 15\n",
      "comprehensive_income_row = 24\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG - TARGET ROW PATTERNS\n",
    "# -----------------------------\n",
    "TARGET_ROW_PATTERNS = {\n",
    "    \"revenue_from_operations\": {\n",
    "        \"keywords\": [\"revenue\", \"operations\"],\n",
    "        \"min_matches\": 2,  # At least 1 keyword must match\n",
    "        \"exclude\": [\"total\", \"other\"]\n",
    "    },\n",
    "    \"total_expenses\": {\n",
    "        \"keywords\": [\"total\", \"expense\", \"expenditure\", \"before\"],\n",
    "        \"min_matches\": 2,  # At least 2 keywords must match\n",
    "        \"exclude\": []\n",
    "    },\n",
    "    \"profit_before_tax\": {\n",
    "        \"keywords\": [\"profit\", \"tax\", \"before\"],\n",
    "        \"min_matches\": 2,\n",
    "        \"exclude\": [\"comprehensive\", \"after\"]\n",
    "    },\n",
    "    \"total_comprehensive_income\": {\n",
    "        \"keywords\": [\"total\", \"comprehensive\", \"income\"],\n",
    "        \"min_matches\": 3,\n",
    "        \"exclude\": [\"other\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# TEXT CLEANING\n",
    "# -----------------------------\n",
    "def clean_row_text(text: str) -> str:\n",
    "    \"\"\"Clean text for row matching\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove markdown formatting\n",
    "    text = re.sub(r'\\*\\*|__|`', '', text)\n",
    "    \n",
    "    # Remove parentheses content like (Refer Note 5), (IV), etc\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    \n",
    "    # Remove common prefixes like \"a.\", \"(a)\", \"i.\", \"1.\", etc\n",
    "    text = re.sub(r'^[a-z]\\.\\s*', '', text)\n",
    "    text = re.sub(r'^\\([a-z]\\)\\s*', '', text)\n",
    "    text = re.sub(r'^\\d+\\.\\s*', '', text)\n",
    "    \n",
    "    # Normalize separators\n",
    "    text = re.sub(r'[/]+', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# FUZZY MATCHING\n",
    "# -----------------------------\n",
    "def matches_pattern(text: str, pattern_config: dict, debug=False) -> bool:\n",
    "    \"\"\"Check if text matches pattern based on keyword count with fuzzy matching.\"\"\"\n",
    "    cleaned = clean_row_text(text)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"      Checking: '{text[:60]}...'\")\n",
    "        print(f\"        Cleaned: '{cleaned[:60]}...'\")\n",
    "    \n",
    "    # Check exclude keywords first\n",
    "    exclude = pattern_config.get(\"exclude\", [])\n",
    "    for keyword in exclude:\n",
    "        if keyword in cleaned:\n",
    "            if debug:\n",
    "                print(f\"        ✗ Contains excluded keyword: '{keyword}'\")\n",
    "            return False\n",
    "    \n",
    "    # Count matching keywords (with fuzzy matching)\n",
    "    keywords = pattern_config.get(\"keywords\", [])\n",
    "    min_matches = pattern_config.get(\"min_matches\", len(keywords))\n",
    "    \n",
    "    matched_count = 0\n",
    "    matched_keywords = []\n",
    "    \n",
    "    words_in_text = cleaned.split()\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        # Direct match - check if keyword is in the cleaned text\n",
    "        if keyword in cleaned:\n",
    "            matched_count += 1\n",
    "            matched_keywords.append(keyword)\n",
    "        # Fuzzy match for typos (e.g., \"xpenses\" matches \"expenses\")\n",
    "        elif len(keyword) > 4:  # Only fuzzy match longer words to avoid false positives\n",
    "            close = get_close_matches(keyword, words_in_text, n=1, cutoff=0.8)\n",
    "            if close:\n",
    "                matched_count += 1\n",
    "                matched_keywords.append(f\"{keyword}≈{close[0]}\")\n",
    "                if debug:\n",
    "                    print(f\"        ~ Fuzzy matched '{keyword}' to '{close[0]}'\")\n",
    "    \n",
    "    if matched_count >= min_matches:\n",
    "        if debug:\n",
    "            print(f\"        ✓ Match! ({matched_count}/{min_matches} keywords: {matched_keywords})\")\n",
    "        return True\n",
    "    else:\n",
    "        if debug:\n",
    "            print(f\"        ✗ Only {matched_count}/{min_matches} keywords matched: {matched_keywords}\")\n",
    "        return False\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN FUNCTION - FIND FIRST OCCURRENCE\n",
    "# -----------------------------\n",
    "def find_row_indices(df, target_patterns=TARGET_ROW_PATTERNS, \n",
    "                     search_columns=5, max_rows=None, debug=True):\n",
    "    \"\"\"\n",
    "    Find row index for first occurrence of each pattern across multiple columns.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        target_patterns: dict of pattern configurations\n",
    "        search_columns: number of columns to search (default 5)\n",
    "        max_rows: max rows to search (None = search all)\n",
    "        debug: print debug info\n",
    "    \n",
    "    Returns:\n",
    "        dict: {pattern_name: row_index}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    max_rows = max_rows or len(df)\n",
    "    max_cols = min(search_columns, len(df.columns))\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Searching for row patterns in first {max_cols} columns...\")\n",
    "        print(f\"Max rows to search: {max_rows}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # For each target pattern, find FIRST occurrence\n",
    "    for pattern_name, pattern_config in target_patterns.items():\n",
    "        found = False\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Searching for: {pattern_name}\")\n",
    "            print(f\"  Keywords: {pattern_config.get('keywords', [])}\")\n",
    "            print(f\"  Min matches: {pattern_config.get('min_matches', 0)}\")\n",
    "            print(f\"  Exclude: {pattern_config.get('exclude', [])}\")\n",
    "        \n",
    "        # Scan rows (stop at first match)\n",
    "        for row_idx in range(min(max_rows, len(df))):\n",
    "            \n",
    "            if found:\n",
    "                break\n",
    "            \n",
    "            # Check across first N columns\n",
    "            for col_idx in range(max_cols):\n",
    "                cell = df.iloc[row_idx, col_idx]\n",
    "                \n",
    "                if pd.isna(cell):\n",
    "                    continue\n",
    "                \n",
    "                cell_str = str(cell).strip()\n",
    "                \n",
    "                if not cell_str:\n",
    "                    continue\n",
    "                \n",
    "                # Check if this cell matches the pattern\n",
    "                if matches_pattern(cell_str, pattern_config, debug=debug):\n",
    "                    result[pattern_name] = row_idx\n",
    "                    found = True\n",
    "                    \n",
    "                    if debug:\n",
    "                        print(f\"  ✓ FOUND at Row {row_idx}, Col {col_idx}\")\n",
    "                        print(f\"    Original text: '{cell_str[:80]}'\\n\")\n",
    "                    \n",
    "                    break  # Stop searching columns for this pattern\n",
    "        \n",
    "        if not found and debug:\n",
    "            print(f\"  ✗ Not found\\n\")\n",
    "    \n",
    "    if debug:\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Final result: {result}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# -----------------------------\n",
    "# CONVENIENCE FUNCTION\n",
    "# -----------------------------\n",
    "def get_row_indices(df, search_columns=5, debug=False):\n",
    "    \"\"\"\n",
    "    Get row indices for common financial statement items.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        search_columns: number of columns to search (default 5)\n",
    "        debug: print debug info\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (revenue_row, expenses_row, pbt_row, comprehensive_income_row)\n",
    "    \"\"\"\n",
    "    rows = find_row_indices(df, search_columns=search_columns, debug=debug)\n",
    "    \n",
    "    revenue_row = rows.get('revenue_from_operations')\n",
    "    expenses_row = rows.get('total_expenses')\n",
    "    pbt_row = rows.get('profit_before_tax')\n",
    "    comprehensive_income_row = rows.get('total_comprehensive_income')\n",
    "    \n",
    "    return revenue_row, expenses_row, pbt_row, comprehensive_income_row\n",
    "\n",
    "# -----------------------------\n",
    "# DIAGNOSTIC HELPER\n",
    "# -----------------------------\n",
    "def show_dataframe_content(df, search_columns=5, max_rows=30):\n",
    "    \"\"\"Show what's actually in the DataFrame\"\"\"\n",
    "    max_cols = min(search_columns, len(df.columns))\n",
    "    max_rows = min(max_rows, len(df))\n",
    "    \n",
    "    #print(\"DataFrame Content (First columns and rows):\")\n",
    "    #print(\"=\"*70)\n",
    "    \n",
    "    for row_idx in range(max_rows):\n",
    "        has_content = False\n",
    "        row_str = f\"Row {row_idx}: \"\n",
    "        \n",
    "        for col_idx in range(max_cols):\n",
    "            cell = df.iloc[row_idx, col_idx]\n",
    "            if pd.notna(cell) and str(cell).strip():\n",
    "                has_content = True\n",
    "                row_str += f\"[Col{col_idx}] '{str(cell)[:40]}...' | \"\n",
    "        \n",
    "        if has_content:\n",
    "            print(row_str)\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# USAGE EXAMPLE\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Test datasaved_md_files/40158ce2-884b-4363-9b6d-24beaaa304ce_9.md\n",
    "    markdown_file_path = \"saved_md_files/ea7c5ca5-0cc1-45f1-84af-51c72e6226ab_11.md\"\n",
    "    # 1. Load the markdown file content\n",
    "    with open(markdown_file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    # 2. Convert text to HTML (ensure 'tables' extension is included)\n",
    "    table = markdown.markdown(text, extensions=['tables'])\n",
    "    # 3. Parse HTML with Pandas\n",
    "    # read_html returns a list of DataFrames, so we take the first one\n",
    "    df = pd.read_html(io.StringIO(table))[0]\n",
    "    #print(df.iloc[0:4,:])\n",
    "\n",
    "    \n",
    "    # Find rows with detailed debug\n",
    "    rows = find_row_indices(df, search_columns=5, debug=False)\n",
    "    \n",
    "    #print(\"\\nFinal Row Indices:\")\n",
    "    #print(f\"revenue_from_operations_row = {rows.get('revenue_from_operations')}\")\n",
    "    #print(f\"total_expenses_row = {rows.get('total_expenses')}\")\n",
    "    #print(f\"profit_before_tax_row = {rows.get('profit_before_tax')}\")\n",
    "    #print(f\"total_comprehensive_income_row = {rows.get('total_comprehensive_income')}\")\n",
    "    \n",
    "    # Or use convenience function\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    revenue_row, expenses_row, pbt_row, comprehensive_income_row = get_row_indices(df, debug=False)\n",
    "    print(\"Using convenience function:\")\n",
    "    print(f\"revenue_row = {revenue_row}\")\n",
    "    print(f\"expenses_row = {expenses_row}\")\n",
    "    print(f\"pbt_row = {pbt_row}\")\n",
    "    print(f\"comprehensive_income_row = {comprehensive_income_row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ed197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7409d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_file_path = \"saved_md_files/3d3c9dfb-537a-491a-a837-23cf5ea1f762_5.md\"\n",
    "# 1. Load the markdown file content\n",
    "with open(markdown_file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "# 2. Convert text to HTML (ensure 'tables' extension is included)\n",
    "table = markdown.markdown(text, extensions=['tables'])\n",
    "# 3. Parse HTML with Pandas\n",
    "# read_html returns a list of DataFrames, so we take the first one\n",
    "df = pd.read_html(io.StringIO(table))[0]\n",
    "#print(df.iloc[0:4,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad84f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Date Columns: {}\n",
      "This Quarter Column: None\n",
      "Previous Quarter Column: None\n",
      "Same Quarter Last Year Column: None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a44a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35570d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niithipeak_s1_backtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
